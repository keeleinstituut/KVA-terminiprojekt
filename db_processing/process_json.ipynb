{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sandra.eiche\\AppData\\Local\\miniconda3\\envs\\private_gpt\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "from typing import List\n",
    "\n",
    "import spacy\n",
    "from document_structure import (Chunk, ContentTextData, Document, FootnoteData,\n",
    "                                TableData, TermData)\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http.models import Distance, PointStruct, VectorParams\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpacySenter:\n",
    "    def __init__(self) -> None:\n",
    "        self.nlp = spacy.load(\"en_core_web_trf\", enable=[\n",
    "                              'transformer', 'parser'])\n",
    "\n",
    "    def get_sentences(self, text: str = '') -> List[dict]:\n",
    "        sentence_data = list()\n",
    "        for sent in self.nlp(text).sents:\n",
    "            sentence_data.append({\n",
    "                'text': sent.text,\n",
    "                'start_char': sent.start_char,\n",
    "                'end_char': sent.end_char,\n",
    "                'length_token': len(sent)\n",
    "            })\n",
    "\n",
    "        return sentence_data\n",
    "\n",
    "\n",
    "class WhitespaceTokenizer:\n",
    "\n",
    "    def get_tokens(self, text: str = '') -> List[dict]:\n",
    "        token_data = list()\n",
    "\n",
    "        tokens = text.split(' ')\n",
    "\n",
    "        char_counter = 0\n",
    "\n",
    "        for token in tokens:\n",
    "            token_data.append({\n",
    "                'text': token,\n",
    "                'start_char': char_counter,\n",
    "                'end_char': char_counter + len(token),\n",
    "            })\n",
    "            char_counter += len(token) + 1\n",
    "        return token_data\n",
    "\n",
    "class E5Tokenizer:\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained('intfloat/multilingual-e5-large')\n",
    "\n",
    "    def get_tokens(self, text: str = '') -> List[dict]:\n",
    "\n",
    "\n",
    "        token_data = list()\n",
    "\n",
    "        encoded_input = self.tokenizer.encode_plus(text, return_offsets_mapping=True, add_special_tokens=False)\n",
    "\n",
    "        # The 'offset_mapping' contains the start and end positions of each token in the original text\n",
    "        offset_mapping = encoded_input['offset_mapping']\n",
    "\n",
    "        for token_index, (start_pos, end_pos) in enumerate(offset_mapping):\n",
    "            token_data.append({\n",
    "                'text': text[start_pos:end_pos],\n",
    "                'start_char': start_pos,\n",
    "                'end_char': end_pos,\n",
    "            })\n",
    "\n",
    "        return token_data\n",
    "\n",
    "def section_chunks_to_points(document_metadata: dict, section_chunks: List[Chunk], last_idx: int, model):\n",
    "    section_points = list()\n",
    "\n",
    "    for i, chunk in enumerate(section_chunks, 1):    \n",
    "        try:\n",
    "            chunk_text = 'passage:' + chunk.get_text()\n",
    "        except TypeError:\n",
    "            print(chunk)\n",
    "            print(chunk_text)\n",
    "            raise TypeError\n",
    "        payload = document_metadata.copy()\n",
    "        payload.update(chunk.get_data())\n",
    "        section_points.append(\n",
    "                PointStruct(id = last_idx + i,\n",
    "                            vector=list(model.encode(chunk_text, normalize_embeddings=True).astype(float)),\n",
    "                            payload=payload)\n",
    "                            )\n",
    "    return section_points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo parameetriks? sh parameetrite suurus, tokenite arv, lausete hulk chunkis, overlap?\n",
    "model = SentenceTransformer('intfloat/multilingual-e5-base')\n",
    "client = QdrantClient(\"172.24.228.4\", port=6333)\n",
    "\n",
    "collection_name = \"test_collection\"\n",
    "embedding_size = 768\n",
    "max_tokens = 450\n",
    "sentence_block_size = 5\n",
    "\n",
    "existing_collections = [coll.name for coll in client.get_collections().collections]\n",
    "\n",
    "if collection_name not in existing_collections:\n",
    "    client.create_collection(\n",
    "        collection_name=collection_name,\n",
    "        vectors_config=VectorParams(size=embedding_size, distance=Distance.COSINE),\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = E5Tokenizer()\n",
    "senter = SpacySenter()\n",
    "\n",
    "\n",
    "fpath = 'C:\\\\Users\\\\sandra.eiche\\\\OneDrive - Eesti Keele Instituut\\\\Documents\\\\KVA\\\\kva_parsed_jsons'\n",
    "fname = '20231004-JDP_0_01_1_2023_Edition_B_web.json'\n",
    "\n",
    "collection_info = client.get_collection(collection_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "operation_id=1 status=<UpdateStatus.ACKNOWLEDGED: 'acknowledged'>\n",
      "operation_id=2 status=<UpdateStatus.ACKNOWLEDGED: 'acknowledged'>\n",
      "operation_id=3 status=<UpdateStatus.ACKNOWLEDGED: 'acknowledged'>\n",
      "operation_id=4 status=<UpdateStatus.ACKNOWLEDGED: 'acknowledged'>\n",
      "operation_id=5 status=<UpdateStatus.ACKNOWLEDGED: 'acknowledged'>\n",
      "operation_id=6 status=<UpdateStatus.ACKNOWLEDGED: 'acknowledged'>\n"
     ]
    }
   ],
   "source": [
    "with open(os.path.join(fpath, fname), 'r') as fin:\n",
    "\n",
    "    document_json = json.loads(fin.read())\n",
    "\n",
    "    document = Document(\n",
    "        json_filename=fname,\n",
    "        filename=document_json['filename'],\n",
    "        publication=document_json['publication'],\n",
    "        publication_year=document_json['publication_year'],\n",
    "        title=document_json['title'],\n",
    "        author=document_json['author'],\n",
    "        languages=document_json['languages'],\n",
    "        field_keywords=document_json['field_keywords'],\n",
    "        header_height=document_json['header_height'],\n",
    "        footer_height=document_json['footer_height'],\n",
    "        table_extraction_strategy=document_json['table_extraction_strategy'],\n",
    "        horizontal_sorting=document_json['horizontal_sorting'],\n",
    "        footnote_regex=document_json['footnote_regex'],\n",
    "        footnote_group=document_json['footnote_group'],\n",
    "        custom_regex=document_json['custom_regex'],\n",
    "        term_data= TermData(document_json['term_data']),\n",
    "        footnote_data=FootnoteData(document_json['footnote_data']),\n",
    "        table_data=TableData(document_json['table_data']),\n",
    "        content_text_data=ContentTextData(document_json['content_text_data'])\n",
    "        )\n",
    "\n",
    "    document_metadata = document.get_metadata()\n",
    "\n",
    "    # parse content chunks one by one \n",
    "    content_chunks = document.content_text_data.to_chunks(sentensizer=senter, tokenizer=tokenizer, \n",
    "                                                          max_tokens=max_tokens, \n",
    "                                                          n_sentences_in_block=sentence_block_size)\n",
    "    term_chunks = document.term_data.to_chunks()\n",
    "    footnote_chunks = document.footnote_data.to_chunks()\n",
    "    table_chunks = document.table_data.to_chunks(tokenizer=tokenizer, max_tokens=max_tokens)\n",
    "\n",
    "\n",
    "    # Chunks to PointStruct\n",
    "    for section_chunks in [content_chunks, term_chunks, footnote_chunks, table_chunks]:\n",
    "        last_idx = client.get_collection(collection_name).vectors_count\n",
    "        if not last_idx:\n",
    "            last_idx = 0\n",
    "\n",
    "        section_points = section_chunks_to_points(document_metadata, section_chunks, last_idx, model=model)\n",
    "        \n",
    "        step = 100\n",
    "        for i in range(0, len(section_points), step): \n",
    "            x = i \n",
    "            operation_info = client.upsert(\n",
    "                collection_name=collection_name,\n",
    "                wait=False,\n",
    "                points=section_points[x:x+step])\n",
    "            print(operation_info)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
